{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSGO Win Probability Model Tuning\n",
    "##### Peter Xenopoulos\n",
    "##### January 31, 2020\n",
    "This Jupyter notebook contains the code for model tuning. You will find the model fitting and tuning procedures for (1) Logistic Regression, (2) XGBoost and (3) CatBoost. Final models are saved to the `models` directory. You can load models using the `.load_model()` method. The data in the `data/` directory is an example of a few matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss, log_loss, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from catboost import Pool, CatBoost, CatBoostClassifier, cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define our random seed and our cutoff date. Matches before and on the cutoff date comprise the training set. Matches after comprise the test set.\n",
    "\n",
    "We also create two lists of columns. The first, without the `_CB` in the name, indicates variables that have been one hot encoded. This is necessary for XGBoost and Logistic Regression.\n",
    "\n",
    "Finally, we read in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "RANDOM_STATE = 2020\n",
    "CUTOFF_DATE = \"2019-06-01\"\n",
    "\n",
    "# One Hot Encoded columns\n",
    "COLS_ALL = [\"MapName_de_dust2\", \"MapName_de_inferno\", \"MapName_de_mirage\", \"MapName_de_nuke\", \"MapName_de_overpass\", \"MapName_de_train\", \"MapName_de_vertigo\", \"BombSite_A\", \"BombSite_B\", \"BombSite_NotPlanted\",\n",
    "       \"TicksSinceStart\", \"CTEqVal\", \"TEqVal\", \"TRemaining\", \"CTRemaining\", \"THpRemaining\", \"CTHpRemaining\", \"BombPlanted\", \"CTDistBombsiteA\", \"CTDistBombsiteB\", \"TDistBombsiteA\", \"TDistBombsiteB\"]\n",
    "# Non-encoded columns\n",
    "COLS_CB_ALL = [\"MapName\", \"BombSite\", \"TicksSinceStart\", \"CTEqVal\", \"TEqVal\", \"TRemaining\", \"CTRemaining\", \"THpRemaining\", \"CTHpRemaining\", \"BombPlanted\", \"CTDistBombsiteA\", \"CTDistBombsiteB\", \"TDistBombsiteA\", \"TDistBombsiteB\"]\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv(\"data/example_matches.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we preprocess the data, creating some one hot encoded columns. Next, we split into the test and train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess columns\n",
    "df[\"GameDate\"] = pd.to_datetime(df[\"GameDate\"])\n",
    "map_names = df[\"MapName\"]\n",
    "df = pd.get_dummies(df, columns = [\"MapName\"], drop_first = False)\n",
    "df[\"MapName\"] = map_names\n",
    "bombsites = df[\"BombSite\"]\n",
    "df = pd.get_dummies(df, columns = [\"BombSite\"], drop_first = False)\n",
    "df[\"BombSite\"] = bombsites\n",
    "\n",
    "# Train/Test split\n",
    "train_df = df[df[\"GameDate\"] < CUTOFF_DATE]\n",
    "test_df = df[df[\"GameDate\"] >= CUTOFF_DATE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split the data into the various design matrices, as well as dropping NAs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data used in logistic regression and XGBoost\n",
    "train_df.dropna(subset = COLS_ALL, inplace=True)\n",
    "X_train_ALL = train_df[COLS_ALL]\n",
    "y_train = train_df[\"CTWin\"]\n",
    "\n",
    "test_df.dropna(subset = COLS_ALL, inplace=True)\n",
    "X_test_ALL = test_df[COLS_ALL]\n",
    "y_test = test_df[\"CTWin\"]\n",
    "\n",
    "# Data used in CatBoost\n",
    "train_df.dropna(subset = COLS_CB_ALL, inplace=True)\n",
    "X_train_cb_ALL = train_df[COLS_CB_ALL]\n",
    "y_train_cb = train_df[\"CTWin\"]\n",
    "\n",
    "test_df.dropna(subset = COLS_CB_ALL, inplace=True)\n",
    "X_test_cb_ALL = test_df[COLS_CB_ALL]\n",
    "y_test_cb = test_df[\"CTWin\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Print Function\n",
    "We use this print function to print the train/test results of our best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_true_labels, y_pred_probs):\n",
    "    \"\"\" Presents performance info\n",
    "    \"\"\"\n",
    "    print(\"---------- LOG LOSS\")\n",
    "    print(log_loss(y_true_labels, y_pred_probs))\n",
    "    print(\"---------- BRIER SCORE\")\n",
    "    print(brier_score_loss(y_true_labels, y_pred_probs[:,1]))\n",
    "    print(\"---------- AUC\")\n",
    "    print(roc_auc_score(y_true_labels, y_pred_probs[:,1]))\n",
    "    print(\"---------- ACCURACY\")\n",
    "    print(accuracy_score(y_true_labels, y_pred_probs[:,1] >= 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Win Rate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses the mean CT win percentage as its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df[\"BaselinePred\"] = train_df.CTWin.mean()\n",
    "print_results(test_df[\"CTWin\"], np.column_stack((1 - test_df[\"BaselinePred\"], test_df[\"BaselinePred\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Average Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses the mean CT win percentage, by map, as its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_map(train, test):\n",
    "    \"\"\" Create a baseline map performance\n",
    "    \"\"\"\n",
    "    # Generate map win percentages\n",
    "    map_win_rate = train.groupby(\"MapName\").CTWin.mean().reset_index()\n",
    "    map_win_rate.columns = [\"MapName\", \"PredWinRate\"]\n",
    "    # Can't do a big join locally, so break it up by map\n",
    "    test[\"PredMapWinRate\"] = 0.5\n",
    "    map_subset_df = []\n",
    "    for map_name in test[\"MapName\"].unique():\n",
    "        subset_map = test[test[\"MapName\"] == map_name]\n",
    "        subset_map[\"PredMapWinRate\"] = map_win_rate[map_win_rate[\"MapName\"] == map_name].PredWinRate.values[0]\n",
    "        map_subset_df.append(subset_map)\n",
    "    test = pd.concat(map_subset_df)\n",
    "    print_results(test[\"CTWin\"], np.column_stack((1 - test[\"PredMapWinRate\"], test[\"PredMapWinRate\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_map(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we present the logistic regression results on two feature sets, one using all features available and the other using no spatial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_scaled = [\"TicksSinceStart\", \"CTEqVal\", \"TEqVal\", \"TRemaining\", \"CTRemaining\", \"THpRemaining\", \"CTHpRemaining\", \"BombPlanted\", \"CTDistBombsiteA\", \"CTDistBombsiteB\", \"TDistBombsiteA\", \"TDistBombsiteB\"]\n",
    "\n",
    "# Train\n",
    "X_train_ALL_scaled = X_train_ALL.copy()\n",
    "features_scaled = X_train_ALL_scaled[cols_scaled]\n",
    "scaler = StandardScaler().fit(features_scaled.values)\n",
    "features_scaled = scaler.transform(features_scaled.values)\n",
    "X_train_ALL_scaled[cols_scaled] = features_scaled\n",
    "\n",
    "# Test\n",
    "X_test_ALL_scaled = X_test_ALL.copy()\n",
    "features_scaled_test = X_test_ALL_scaled[cols_scaled]\n",
    "features_scaled_test = scaler.transform(features_scaled_test.values)\n",
    "X_test_ALL_scaled[cols_scaled] = features_scaled_test\n",
    "\n",
    "lr_all = LogisticRegression(random_state=2020, penalty=\"none\", solver=\"saga\").fit(X_train_ALL_scaled, y_train)\n",
    "lr_all_test_probs = lr_all.predict_proba(X_test_ALL_scaled)\n",
    "\n",
    "print_results(y_test, lr_all_test_probs)\n",
    "\n",
    "lr_filename = \"models/logreg.model\"\n",
    "pickle.dump(lr_all, open(lr_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we present the XGBoost results on two feature sets, one using all features available and the other using no spatial features.\n",
    "\n",
    "We search over the parameter space below, using grid search with 5-fold cross validation and a log loss scoring metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = { \n",
    " \"max_depth\"        : [6, 8, 10, 12, 14],\n",
    " \"colsample_bytree\" : [0.2, 0.4, 0.6, 0.8],\n",
    " \"learning_rate\"    : [0.01, 0.05, 0.1, 0.2],\n",
    " \"min_child_weight\" : [1, 3, 5, 7]}\n",
    "\n",
    "xgb_kfold = KFold(n_splits=5, random_state=RANDOM_STATE, shuffle=True)\n",
    "xgb = XGBClassifier(n_estimators=100, objective=\"binary:logistic\", tree_method=\"gpu_hist\", gpu_id=0)\n",
    "\n",
    "xgb_cv = GridSearchCV(xgb, param_grid=xgb_params, cv=xgb_kfold, scoring=\"neg_log_loss\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv.fit(X_train_ALL, y_train)\n",
    "xgb_all = xgb_cv.best_estimator_\n",
    "xgb_all.save_model(\"models/xgboost.model\")\n",
    "print(xgb_cv.best_params_)\n",
    "xgb_all_test_probs = xgb_all.predict_proba(X_test_ALL)\n",
    "print_results(y_test, xgb_all_test_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we present the XGBoost results on two feature sets, one using all features available and the other using no spatial features.\n",
    "\n",
    "We search over the parameter space below, using grid search with 5-fold cross validation and a log loss scoring metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_params = {\n",
    "    \"learning_rate\": [0.05, 0.1, 0.5, 1],\n",
    "    \"depth\": [6, 8, 10, 12, 14],\n",
    "    \"l2_leaf_reg\": [1, 3, 5, 7, 9]\n",
    "}\n",
    "\n",
    "cb_kfold = KFold(n_splits=5, random_state=RANDOM_STATE, shuffle=True)\n",
    "cb_all_pool = Pool(data = X_train_cb_ALL, label = y_train_cb, cat_features = [0, 1])\n",
    "cb_no_spatial_pool = Pool(data = X_train_cb_NO_SPATIAL, label = y_train_cb, cat_features = [0, 1])\n",
    "\n",
    "cb = CatBoostClassifier(iterations=100, task_type=\"GPU\", devices=\"0:1\", custom_metric=[\"Logloss\"])\n",
    "\n",
    "cb_cv = GridSearchCV(cb, param_grid=cb_params, cv=cb_kfold, scoring=\"neg_log_loss\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_cv.fit(X_train_cb_ALL, y_train, cat_features = [0, 1])\n",
    "cb_all = cb_cv.best_estimator_\n",
    "cb_all.save_model(\"models/catboost.model\", \"cbm\")\n",
    "print(cb_cv.best_params_)\n",
    "cb_all_test_probs = cb_all.predict_proba(X_test_cb_ALL)\n",
    "print_results(y_test, cb_all_test_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
